

import numpy as np
x= np.array([0.8, 0.2, 0.3])
w= np.array([[0.2, 0.3],[0.5, 0.2],[0.3, 0.6]])
b= np.array([0.02,0.01])

z= np.dot(x, w) + b
print("Output: ", z)

def sigmoid(x):
  return 1/ (1+np.exp(-x))

a=sigmoid(z)
print("Activated Output: ", a)
     
Output:  [0.37 0.47]
Activated Output:  [0.59145898 0.61538376]

w2= np.array([[0.6],[0.3]])
b2= np.array([0.03])
z2= np.dot(a,w2)+b
a2=sigmoid(z2)
print("Final Output: ", a2)
     
Final Output:  [0.63633465 0.63401738]

def relu(x):
  return np.maximum(0, x)

a_relu = relu(z)
print("ReLU Activated Output: ", a_relu)
a2_relu = relu(z2)
print("ReLU Final Output: ", a2_relu)
     
ReLU Activated Output:  [0.37 0.47]
ReLU Final Output:  [0.55949051 0.54949051]

y=1
loss = (y-a2)**2 #mean squared loss function
print("Loss: ",loss)
     
Loss:  [0.13225249 0.13394328]

lr = 0.2
dloss_da2 = -2*(y - a2)
da2_dz2 = a2*(1-a2)
dz2_dw2 = a
grad_w2 = dloss_da2 * da2_dz2 * dz2_dw2
w2 = w2 - lr * grad_w2.reshape(2,1) # Corrected reshape to (2,1)
print("Updated W2: ", w2)
     
Updated W2:  [[0.61991013]
 [0.32090394]]